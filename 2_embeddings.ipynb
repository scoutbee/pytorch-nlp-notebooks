{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "303nHJfnVCoR",
    "outputId": "958788ff-f5bb-414e-e238-172c7d2bf543"
   },
   "outputs": [],
   "source": [
    "!pip install pymagnitude pytorch_pretrained_bert -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_H68bmuYVBQA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pymagnitude import Magnitude\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from scipy import spatial\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "\n",
    "RED, BLUE = '#FF4136', '#0074D9'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "17XKXlnyVBQF"
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "An embedding maps discrete, categorical values to a continous space. Major advances in NLP applications have come from these continuous representations of words.\n",
    "\n",
    "If we have some sentence,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fxTuHmKiVBQH",
    "outputId": "80aa0168-4101-4518-bf18-0b93dd99e72c"
   },
   "outputs": [],
   "source": [
    "sentence = 'the quick brown fox jumps over the lazy dog'\n",
    "words = sentence.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iKfSGpJxVBQP"
   },
   "source": [
    "We first turn this sentence into numbers by assigning each unique word an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "LWXE9OmDVBQR",
    "outputId": "8c2f8b02-8048-4f9c-cf9f-f8f0362aeb45"
   },
   "outputs": [],
   "source": [
    "word2idx = {word: idx for idx, word in enumerate(sorted(set(words)))}\n",
    "word2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R8plCT7bVBQX"
   },
   "source": [
    "Then, we turn each word in our sentence into its assigned index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aB40yyPSVBQY",
    "outputId": "f89d14eb-7b1b-4bd3-bc4d-f9a958fd91c8"
   },
   "outputs": [],
   "source": [
    "idxs = torch.LongTensor([word2idx[word] for word in sentence.split()])\n",
    "idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4xE--RIrVBQe"
   },
   "source": [
    "Next, we want to create an **embedding layer**. The embedding layer is a 2-D matrix of shape `(n_vocab x embedding_dimension)`. If we apply our input list of indices to the embedding layer, each value in the input list of indices maps to that specific row of the embedding layer matrix. The output shape after applying the input list of indices to the embedding layer is another 2-D matrix of shape `(n_words x embedding_dimension)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "VlF7QIr5VBQg",
    "outputId": "87945096-f100-45a6-c42f-5d8de4b6e923"
   },
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(num_embeddings=len(word2idx), embedding_dim=3)\n",
    "embeddings = embedding_layer(idxs)\n",
    "embeddings, embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5G_N4Cb0VBQl"
   },
   "source": [
    "The PyTorch builtin embedding layer comes with randomly initialized weights that are updated with gradient descent as your model learns to map input indices to some kind of output. However, often it is better to use pretrained embeddings that do not update but instead are frozen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWFKrgx-VBQm"
   },
   "source": [
    "## GloVe Embeddings\n",
    "\n",
    "GloVe embeddings are one of the most popular pretrained word embeddings in use. You can download them [here](https://nlp.stanford.edu/projects/glove/). For the best performance for most applications, I recommend using their Common Crawl embeddings with 840B tokens; however, they take the longest to download, so instead let's download the Wikipedia embeddings with 6B tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FKo_Pg6wVBQn"
   },
   "outputs": [],
   "source": [
    "# Download GloVe vectors (uncomment the below)\n",
    "\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip && unzip glove.6B.zip && mkdir glove && mv glove*.txt glove\n",
    "\n",
    "# GLOVE_FILENAME = 'glove/glove.6B.50d.txt'\n",
    "# glove_index = {}\n",
    "# n_lines = sum(1 for line in open(GLOVE_FILENAME))\n",
    "# with open(GLOVE_FILENAME) as fp:\n",
    "#     for line in tqdm(fp, total=n_lines):\n",
    "#         split = line.split()\n",
    "#         word = split[0]\n",
    "#         vector = np.array(split[1:]).astype(float)\n",
    "#         glove_index[word] = vector\n",
    "        \n",
    "# glove_embeddings = np.array([glove_index[word] for word in words])\n",
    "\n",
    "# # Because the length of the input sequence is 9 words and the embedding\n",
    "# # dimension is 50, the output shape is `(9 x 50)`.\n",
    "# glove_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2StD14zGVBQ3"
   },
   "source": [
    "### Magnitude Library for Fast Vector Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvyAGoEIVBQ4"
   },
   "source": [
    "Loading the entire GloVe file can take up a lot of memory. We can use the `magnitude` library for more efficient embedding vector loading. You can download the magnitude version of GloVe embeddings [here](https://github.com/plasticityai/magnitude#pre-converted-magnitude-formats-of-popular-embeddings-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "vnzGlMubVBQ5",
    "outputId": "856e5cad-bd31-4b2d-a516-311dc63d886e"
   },
   "outputs": [],
   "source": [
    "!wget http://magnitude.plasticity.ai/glove/light/glove.6B.50d.magnitude glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w-0r7FHLVBQ-"
   },
   "outputs": [],
   "source": [
    "# Load Magnitude GloVe vectors\n",
    "glove_vectors = Magnitude('glove/glove.6B.50d.magnitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DP2sOnZ1VBRC"
   },
   "outputs": [],
   "source": [
    "glove_embeddings = glove_vectors.query(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ARcZ2PwsVBRG"
   },
   "source": [
    "## Similarity operations on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Ara5883VBRH"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(word1, word2):\n",
    "    vector1, vector2 = glove_vectors.query(word1), glove_vectors.query(word2)\n",
    "    return 1 - spatial.distance.cosine(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "LQV1Ur9PVBRO",
    "outputId": "e8b5586b-ebd1-4026-8f1c-b2fb1bb108d8"
   },
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    ('dog', 'cat'),\n",
    "    ('tree', 'cat'),\n",
    "    ('tree', 'leaf'),\n",
    "    ('king', 'queen'),\n",
    "]\n",
    "\n",
    "for word1, word2 in word_pairs:\n",
    "    print(f'Similarity between \"{word1}\" and \"{word2}\":\\t{cosine_similarity(word1, word2):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3mvCSt-2VBRV"
   },
   "source": [
    "## Visualizing Embeddings\n",
    "\n",
    "We can demonstrate that embeddings carry semantic information by plotting them. However, because our embeddings are more than three dimensions, they are impossible to visualize. Therefore, we can use an algorithm called t-SNE to project the word embeddings to a lower dimension in order to plot them in 2-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MYoO6T2kVBRX"
   },
   "outputs": [],
   "source": [
    "ANIMALS = [\n",
    "    'whale',\n",
    "    'fish',\n",
    "    'horse',\n",
    "    'rabbit',\n",
    "    'sheep',\n",
    "    'lion',\n",
    "    'dog',\n",
    "    'cat',\n",
    "    'tiger',\n",
    "    'hamster',\n",
    "    'pig',\n",
    "    'goat',\n",
    "    'lizard',\n",
    "    'elephant',\n",
    "    'giraffe',\n",
    "    'hippo',\n",
    "    'zebra',\n",
    "]\n",
    "\n",
    "HOUSEHOLD_OBJECTS = [\n",
    "    'stapler',\n",
    "    'screw',\n",
    "    'nail',\n",
    "    'tv',\n",
    "    'dresser',\n",
    "    'keyboard',\n",
    "    'hairdryer',\n",
    "    'couch',\n",
    "    'sofa',\n",
    "    'lamp',\n",
    "    'chair',\n",
    "    'desk',\n",
    "    'pen',\n",
    "    'pencil',\n",
    "    'table',\n",
    "    'sock',\n",
    "    'floor',\n",
    "    'wall',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5R_k2AiCVBRd",
    "outputId": "584ff6fa-2a77-41fc-f18c-1c6662f761a5"
   },
   "outputs": [],
   "source": [
    "tsne_words_embedded = TSNE(n_components=2).fit_transform(glove_vectors.query(ANIMALS + HOUSEHOLD_OBJECTS))\n",
    "tsne_words_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "colab_type": "code",
    "id": "OfM7fFagVBRh",
    "outputId": "c61f7d22-aafe-4256-8900-a4b8c12fd4d2"
   },
   "outputs": [],
   "source": [
    "x, y = zip(*tsne_words_embedded)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for i, label in enumerate(ANIMALS + HOUSEHOLD_OBJECTS):\n",
    "    if label in ANIMALS:\n",
    "        color = BLUE\n",
    "    elif label in HOUSEHOLD_OBJECTS:\n",
    "        color = RED\n",
    "        \n",
    "    ax.scatter(x[i], y[i], c=color)\n",
    "    ax.annotate(label, (x[i], y[i]))\n",
    "\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IFfVbmhfVBRl"
   },
   "source": [
    "## Context embeddings\n",
    "\n",
    "GloVe and Fasttext are two examples of global embeddings, where the embeddings don't change even though the \"sense\" of the word might change given the context. This can be a problem for cases such as:\n",
    "\n",
    "- A **mouse** stole some cheese.\n",
    "- I bought a new **mouse** the other day for my computer.\n",
    "\n",
    "The word mouse can mean both an animal and a computer accessory depending on the context, yet for GloVe they would receive the same exact distributed representation. We can combat this by taking into account the surroudning words to create a context-sensitive embedding. Context embeddings such as Bert are really popular right now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "v2Kqxd54VBRm",
    "outputId": "31ab487f-bc09-4a6f-a383-ae8ea371c380"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "def to_bert_embeddings(text, return_tokens=False):\n",
    "    if isinstance(text, list):\n",
    "        # Already tokenized\n",
    "        tokens = tokenizer.tokenize(' '.join(text))\n",
    "    else:\n",
    "        # Need to tokenize\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        \n",
    "    tokens_with_tags = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    indices = tokenizer.convert_tokens_to_ids(tokens_with_tags)\n",
    "\n",
    "    out = model(torch.LongTensor(indices).unsqueeze(0))\n",
    "    \n",
    "    # Concatenate the last four layers and use that as the embedding\n",
    "    # source: https://jalammar.github.io/illustrated-bert/\n",
    "    embeddings_matrix = torch.stack(out[0]).squeeze(1)[-4:]  # use last 4 layers\n",
    "    embeddings = []\n",
    "    for j in range(embeddings_matrix.shape[1]):\n",
    "        embeddings.append(embeddings_matrix[:, j, :].flatten().detach().numpy())\n",
    "        \n",
    "    # Ignore [CLS] and [SEP]\n",
    "    embeddings = embeddings[1:-1]\n",
    "        \n",
    "    if return_tokens:\n",
    "        assert len(embeddings) == len(tokens)\n",
    "        return embeddings, tokens\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6PAaDILVBRq"
   },
   "outputs": [],
   "source": [
    "words_sentences = [\n",
    "    ('mouse', 'I saw a mouse run off with some cheese.'),\n",
    "    ('mouse', 'I bought a new computer mouse yesterday.'),\n",
    "    ('cat', 'My cat jumped on the bed.'),\n",
    "    ('keyboard', 'My computer keyboard broke when I spilled juice on it.'),\n",
    "    ('dessert', 'I had a banana fudge sunday for dessert.'),\n",
    "    ('dinner', 'What did you eat for dinner?'),\n",
    "    ('lunch', 'Yesterday I had a bacon lettuce tomato sandwich for lunch. It was tasty!'),\n",
    "    ('computer', 'My computer broke after the motherdrive was overloaded.'),\n",
    "    ('program', 'I like to program in Java and Python.'),\n",
    "    ('pasta', 'I like to put tomatoes and cheese in my pasta.'),\n",
    "]\n",
    "words = [words_sentence[0] for words_sentence in words_sentences]\n",
    "sentences = [words_sentence[1] for words_sentence in words_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVSuEP8fVBRt"
   },
   "outputs": [],
   "source": [
    "embeddings_lst, tokens_lst = zip(*[to_bert_embeddings(sentence, return_tokens=True) for sentence in sentences])\n",
    "words, tokens_lst, embeddings_lst = zip(*[(word, tokens, embeddings) for word, tokens, embeddings in zip(words, tokens_lst, embeddings_lst) if word in tokens])\n",
    "\n",
    "# Convert tuples to lists\n",
    "words, tokens_lst, tokens_lst = map(list, [words, tokens_lst, tokens_lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SBCrt11cVBRw"
   },
   "outputs": [],
   "source": [
    "target_indices = [tokens.index(word) for word, tokens in zip(words, tokens_lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IT7nqNYbVBRz"
   },
   "outputs": [],
   "source": [
    "target_embeddings = [embeddings[idx] for idx, embeddings in zip(target_indices, embeddings_lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787
    },
    "colab_type": "code",
    "id": "_x17Kq7mVBR1",
    "outputId": "5314af3f-fe22-46cf-b0ce-df0f9be84682"
   },
   "outputs": [],
   "source": [
    "tsne_words_embedded = TSNE(n_components=2).fit_transform(target_embeddings)\n",
    "x, y = zip(*tsne_words_embedded)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 10))\n",
    "\n",
    "for word, tokens, x_i, y_i in zip(words, tokens_lst, x, y):\n",
    "    ax.scatter(x_i, y_i, c=RED)\n",
    "    ax.annotate(' '.join([f'$\\\\bf{x}$' if x == word else x for x in tokens]), (x_i, y_i))\n",
    "\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x64xA81sVBR6"
   },
   "source": [
    "## Try-it-yourself\n",
    "\n",
    "- Use the Magnitude library to load other pretrained embeddings such as Fasttext\n",
    "- Try comparing the GloVe embeddings with the Fasttext embeddings by making t-SNE plots of both, or checking the similarity scores between the same set of words\n",
    "- Make t-SNE plots using your own words and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QDP37tWKVBR7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "2_embeddings.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
