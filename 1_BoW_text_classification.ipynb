{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Bag of Words Text Classification</h1></center>\n",
    "\n",
    "In this tutorial we will show how to build a simple Bag of Words (BoW) text classifier using PyTorch. The classifier is trained on IMDB movie reviews dataset. \n",
    "\n",
    "\n",
    "<h4>\n",
    "The concepts covered in this tutorial are: \n",
    "<br>\n",
    "<br> 1. NLP text <i><b>pre-processing</b></i>\n",
    "<br>\n",
    "<br> 2. Split of <i><b>training, validation and testing datasets</b></i>\n",
    "<br>\n",
    "<br> 3. How to build a simple <i><b>feed-forward neural network</b></i> using PyTorch \n",
    "<br>\n",
    "<br> 4. How different <i><b>optimizer</b></i> affects learning rate and convergence to global minimum \n",
    "<br>\n",
    "<br> 5. <i><b>Under-fitting v.s. Over-fitting</b></i> \n",
    "<br>\n",
    "<br> 6. <i><b>BoW</b></i> text classifier \n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/jeffrey/envs/pytorch-nlp-notebooks/lib/python3.6/site-packages (3.4)\n",
      "Requirement already satisfied: singledispatch in /Users/jeffrey/envs/pytorch-nlp-notebooks/lib/python3.6/site-packages (from nltk) (3.4.0.3)\n",
      "Requirement already satisfied: six in /Users/jeffrey/envs/pytorch-nlp-notebooks/lib/python3.6/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: tqdm in /Users/jeffrey/envs/pytorch-nlp-notebooks/lib/python3.6/site-packages (4.31.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # regular expression\n",
    "from collections import Counter \n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "from IPython.core.display import display, HTML\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "# PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# nltk text processors\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from pypeln import process as pr # multi-processing\n",
    "from tqdm import tqdm, tqdm_notebook # show progress bar\n",
    "\n",
    "%matplotlib inline\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jeffrey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jeffrey/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = 'data/imdb_reviews.csv'\n",
    "# if not Path(DATA_PATH).is_file():\n",
    "#     gdd.download_file_from_google_drive(\n",
    "#         file_id='1zfM5E6HvKIe7f3rEt1V2gBpw5QOSSKQz',\n",
    "#         dest_path=DATA_PATH,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run locally\n",
    "DATA_PATH = '/Users/jeffrey/Downloads/imdb_reviews.csv'\n",
    "df = pd.read_csv(\n",
    "    DATA_PATH,\n",
    "    encoding='ISO-8859-1',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a look at a few examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Seeing this film for the first time twenty yea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12361</th>\n",
       "      <td>I went and saw this movie last night after bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  label\n",
       "55     Seeing this film for the first time twenty yea...      0\n",
       "12361  I went and saw this movie last night after bei...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[[55, 12361], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 62155 \n",
      "\n",
      "Negative review:\n",
      "Seeing this film for the first time twenty years after its release I don't quite get it. Why has this been such a huge hit in 1986? Its amateurishness drips from every scene. The jokes are lame and predictable. The sex scenes are exploitative and over the top (that is not to say that Miss Rudnik does not have nice boobs!). The singing is \"schrecklich\". The only genuinely funny scene is the big shoot out when the gangsters die break dancing, a trait that dates the movie firmly to the mid-eighties. It's really quite puzzling to me how incapable I am to grasp what evoked the enthusiasm of the cheering audiences in 1986 (and apparently still today, reading my fellow IMDBers comments). \n",
      "\n",
      "Positive review:\n",
      "I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Number of records:', len(df), '\\n')\n",
    "print('Negative review:')\n",
    "print(df.loc[55,].review, '\\n')\n",
    "print('Positive review:')\n",
    "print(df.loc[12361,].review, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text\n",
    "\n",
    "* Replace weird characters\n",
    "* Lowercase\n",
    "* Tokenize \n",
    "* Stemming & Lemmatize\n",
    "* Remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see how to pre-process these steps one by one. Below I constructed a test corpus which composed of 3 reviews. Each review is a paragraph.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yesterday I gathered 184 datasets for training, but I just found out they're not useful at all!!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus = \"Yesterday I gathered 184 datasets for training, but I just found out they're not useful at all!!\"\n",
    "test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yesterday i gathered 184 datasets for training but i just found out theyre not useful at all'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove special characters & lowercase\n",
    "clean_corpus = re.sub(r'[^\\w\\s]', '', test_corpus)\n",
    "clean_corpus = clean_corpus.lower()\n",
    "clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yesterday', 'i', 'gathered', '184', 'datasets', 'for', 'training', 'but', 'i', 'just', 'found', 'out', 'theyre', 'not', 'useful', 'at', 'all']\n"
     ]
    }
   ],
   "source": [
    "# tokenize\n",
    "clean_tokens = wordpunct_tokenize(clean_corpus)\n",
    "print(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yesterday', 'i', 'gather', '184', 'datasets', 'for', 'train', 'but', 'i', 'just', 'find', 'out', 'theyre', 'not', 'useful', 'at', 'all']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "clean_tokens = [lemmatizer.lemmatize(token) for token in clean_tokens]\n",
    "clean_tokens = [lemmatizer.lemmatize(token, \"v\") for token in clean_tokens]\n",
    "print(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yesterday', 'i', 'gather', '<NUM>', 'datasets', 'for', 'train', 'but', 'i', 'just', 'find', 'out', 'theyre', 'not', 'useful', 'at', 'all']\n"
     ]
    }
   ],
   "source": [
    "clean_tokens = [re.sub(r'[0-9]+', '<NUM>', token) for token in clean_tokens]\n",
    "print(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yesterday', 'gather', '<NUM>', 'datasets', 'train', 'find', 'theyre', 'useful']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "clean_tokens = [token for token in clean_tokens if token not in stop_words]\n",
    "print(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yesterday': 0,\n",
       " 'gather': 1,\n",
       " '<NUM>': 2,\n",
       " 'datasets': 3,\n",
       " 'train': 4,\n",
       " 'find': 5,\n",
       " 'theyre': 6,\n",
       " 'useful': 7}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocab(corpus):\n",
    "    vocab = {}\n",
    "    for doc in corpus:\n",
    "        for token in doc:\n",
    "            if token not in vocab.keys():\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "build_vocab([clean_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'yesterday',\n",
       " 1: 'gather',\n",
       " 2: '<NUM>',\n",
       " 3: 'datasets',\n",
       " 4: 'train',\n",
       " 5: 'find',\n",
       " 6: 'theyre',\n",
       " 7: 'useful'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_index2token(vocab):\n",
    "    index2token = {}\n",
    "    for token in vocab.keys():\n",
    "        index2token[vocab[token]] = token\n",
    "    return index2token\n",
    "\n",
    "build_index2token(build_vocab([clean_tokens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's pacakage the pre-processing steps together into functions and apply on our dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare_words(tokens, common_tokens, max_len):\n",
    "    return [token if token in common_tokens else '<UNK>' for token in tokens][-max_len:]\n",
    "\n",
    "def replace_numbers(tokens):\n",
    "    return [re.sub(r'[0-9]+', '<NUM>', token) for token in tokens]\n",
    "\n",
    "def tokenize(text, stop_words, lemmatizer):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # remove special characters\n",
    "    text = text.lower() # lowercase\n",
    "    tokens = wordpunct_tokenize(text) # tokenize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # noun lemmatizer\n",
    "    tokens = [lemmatizer.lemmatize(token, \"v\") for token in tokens] # verb lemmatizer\n",
    "    tokens = [token for token in tokens if token not in stop_words] # remove stopwords\n",
    "    return tokens\n",
    "\n",
    "def build_bow_vector(sequence, idx2token):\n",
    "    vector = [0] * len(idx2token)\n",
    "    for token_idx in sequence:\n",
    "        if token_idx not in idx2token:\n",
    "            raise ValueError('Wrong sequence index found!')\n",
    "        else:\n",
    "            vector[token_idx] += 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "N_WORKERS = 10\n",
    "MAX_LEN = 128\n",
    "MAX_VOCAB = 8000\n",
    "\n",
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, data_path, max_vocab=5000, max_len=128):\n",
    "        df = pd.read_csv(data_path)\n",
    "        \n",
    "        # Clean and tokenize\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stage = pr.map(\n",
    "            partial(\n",
    "                tokenize,\n",
    "                stop_words=stop_words,\n",
    "                lemmatizer=lemmatizer,\n",
    "            ),\n",
    "            df.review.tolist(), \n",
    "            workers=N_WORKERS,\n",
    "        )\n",
    "        df['tokens'] = list(x for x in tqdm(stage, total=len(df)))    \n",
    "        \n",
    "        all_tokens = [token for doc in list(df.tokens) for token in doc]\n",
    "        \n",
    "        # Build most common tokens bound by max vocab size\n",
    "        common_tokens = set( \n",
    "            list(\n",
    "                zip(*Counter(all_tokens).most_common(max_vocab))\n",
    "            )[0] \n",
    "        )\n",
    "        \n",
    "        # Replace rare words with <UNK>\n",
    "        stage = pr.map(\n",
    "            partial(\n",
    "                remove_rare_words,\n",
    "                common_tokens=common_tokens,\n",
    "                max_len=max_len\n",
    "            ), \n",
    "            df.tokens.tolist(), \n",
    "            workers=N_WORKERS,\n",
    "        )\n",
    "        df.loc[:, 'tokens'] = list(x for x in tqdm(stage, total=len(df.tokens)))\n",
    "        \n",
    "        # Replace numbers with <NUM>\n",
    "        stage = pr.map(\n",
    "            partial(\n",
    "                replace_numbers,\n",
    "            ), \n",
    "            df.tokens.tolist(), \n",
    "            workers=N_WORKERS,\n",
    "        )\n",
    "        df.loc[:, 'tokens'] = list(x for x in tqdm(stage, total=len(df.tokens)))\n",
    "        \n",
    "        # Remove sequences with only <UNK>\n",
    "        stage = pr.map(\n",
    "            lambda tokens: any(token != '<UNK>' for token in tokens), \n",
    "            df.tokens.tolist(), \n",
    "            workers=N_WORKERS,\n",
    "        )\n",
    "        df = df[list(x for x in tqdm(stage, total=len(df.tokens)))]\n",
    "        \n",
    "        # Build vocab\n",
    "        vocab = sorted(set(\n",
    "            token for doc in list(df.tokens) for token in doc\n",
    "        ))\n",
    "        self.token2idx = {token: idx for idx, token in enumerate(vocab)}\n",
    "        self.idx2token = {idx: token for token, idx in self.token2idx.items()}\n",
    "        \n",
    "        # Convert tokens to indexes\n",
    "        df['indexed_tokens'] = df.tokens.apply(\n",
    "            lambda doc: [self.token2idx[token] for token in doc],\n",
    "        )\n",
    "        \n",
    "        # Build BoW vector\n",
    "        df['bow_vector'] = df.indexed_tokens.apply(\n",
    "            build_bow_vector, args=(self.idx2token,)\n",
    "        )\n",
    "        \n",
    "        # Build TF-IDF vector\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            analyzer='word',\n",
    "            tokenizer=lambda doc: doc,\n",
    "            preprocessor=lambda doc: doc,\n",
    "            token_pattern=None,\n",
    "        )\n",
    "        vectors = vectorizer.fit_transform(df.tokens).toarray()\n",
    "        df['tfidf_vector'] = [vector.tolist() for vector in vectors]\n",
    "        \n",
    "        self.text = df.review.tolist()\n",
    "        self.sequences = df.indexed_tokens.tolist()\n",
    "        self.bow_vector = df.bow_vector.tolist()\n",
    "        self.tfidf_vector = df.tfidf_vector.tolist()\n",
    "        self.targets = df.label.tolist()\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return (\n",
    "            self.sequences[i],\n",
    "            self.bow_vector[i],\n",
    "            self.tfidf_vector[i],\n",
    "            self.targets[i],\n",
    "            self.text[i],\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62155/62155 [01:37<00:00, 637.14it/s] \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62155/62155 [00:29<00:00, 2096.85it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62155/62155 [00:29<00:00, 2142.84it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62155/62155 [00:22<00:00, 2763.91it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = ImdbDataset(DATA_PATH, max_vocab=MAX_VOCAB, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See a random sample out of the dataset processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 62155 \n",
      "\n",
      "index: 21322 \n",
      "\n",
      "(spoilers?)<br /><br />I've heard some gripe about the special effects. But that should detract from the movie. THe movie is a suspense film. And it's very good at that. So from that stand point, this movie rocks. Franke rocks. Enjoy to one's plastic hearts content. So no complaints for this movie. Unless you watch the english dub, which is a total farce. It creates the illusion it's a B movie. <br /><br />One complaint I do have is the music video on the dvd. It doesn't say who sings it. I'd love to know. <br /><br />8/10<br /><br />Quality: 10/10 Entertainment : 10/10 Replayable: 5/10 \n",
      "\n",
      "[1119, 1561, 4926, 2162, 2581, 2331, 4695, 6961, 8, 3162, 1829, 7560, 2451, 6148, 8, 7038, 3726, 816, 7560, 2442, 624, 3773, 4446, 6261, 8, 2218, 8, 8, 8, 1933, 5240, 5134, 991, 7501, 3570, 8, 3397, 3386, 3921, 8, 8, 7818, 291, 4897, 2710, 8, 8, 8, 3294, 1990, 1119, 3055, 6085, 6085, 6208, 8, 2325, 8, 7259, 3162, 7818, 1119, 4731, 5254, 1746, 7594, 7259, 2044, 8, 2563, 6426, 968, 2854, 816, 4016, 223, 861, 728, 3420, 6012, 2286, 1279, 2649, 8, 8, 6753, 3448, 8, 5797, 2633, 5125, 4729, 16, 1112, 5524, 7738, 839, 5641, 6006, 609, 7605, 7098, 5641, 6309, 8, 6931, 6503, 6999, 4986, 1752, 816, 5611, 1263, 2304, 6377, 7744, 8, 8, 609, 7027, 4638, 5240, 3921, 7834, 2324, 1263, 2853, 2649] \n",
      "\n",
      "BoW vector size: 7851 \n",
      "\n",
      "TF-IDF vector size: 7851 \n",
      "\n",
      "Sentiment: 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Number of records:', len(dataset), '\\n')\n",
    "\n",
    "import random\n",
    "random_idx = random.randint(0,len(dataset)-1)\n",
    "print('index:', random_idx, '\\n')\n",
    "sample_seq, bow_vector, tfidf_vector, sample_target, sample_text = dataset[random_idx]\n",
    "print(sample_text, '\\n')\n",
    "print(sample_seq, '\\n')\n",
    "print('BoW vector size:', len(bow_vector), '\\n')\n",
    "print('TF-IDF vector size:', len(tfidf_vector), '\\n')\n",
    "print('Sentiment:', sample_target, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training, validation, and test sets\n",
    "\n",
    "- **Training**: data the model learns from\n",
    "- **Validation**: data to evaluate with for hyperparameter tuning (make sure the model doesn't overfit!)\n",
    "- **Testing**: data to evaluate the final performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_valid_test(corpus, valid_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"Split dataset into train, validation, and test.\"\"\"\n",
    "    test_length = int(len(corpus) * test_ratio)\n",
    "    valid_length = int(len(corpus) * valid_ratio)\n",
    "    train_length = len(corpus) - valid_length - test_length\n",
    "    return random_split(\n",
    "        corpus, lengths=[train_length, valid_length, test_length],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55941, 3107, 3107)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, valid_dataset, test_dataset = split_train_valid_test(\n",
    "    dataset, valid_ratio=0.05, test_ratio=0.05)\n",
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 528\n",
    "\n",
    "def collate(batch):\n",
    "    seq = [item[0] for item in batch]\n",
    "    bow = [item[1] for item in batch]\n",
    "    tfidf = [item[2] for item in batch]\n",
    "    target = torch.LongTensor([item[3] for item in batch])\n",
    "    text = [item[4] for item in batch]\n",
    "    return seq, bow, tfidf, target, text\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training batches: 106 \n",
      "\n",
      "Training input sequence: [7605, 4607, 6148, 1377, 3482, 2408, 1522, 7704, 7560, 7360, 3386, 4607, 7582, 4087, 4607, 7560, 7289, 7116, 816, 4187, 6712, 7560, 2996, 3386, 406, 7560, 5555, 526, 6712, 7560, 7825, 4278, 2612, 4187, 2953, 6809, 8, 7771, 668, 4608, 816, 1009, 1092, 6738, 8, 7647, 5074, 4096, 2530, 7560, 5555, 5875, 2044, 3175, 8, 8, 4005, 8, 4824, 2978, 5947, 5967, 2608, 1722, 5082, 7737, 6144, 2379, 2125, 8, 5986, 5947, 6113, 328, 5645, 8, 5082, 2649, 7510, 7088, 6637, 5317, 2843, 1298, 329, 7808, 4773, 4269, 4616, 6178, 816, 4607, 2408, 75, 3735, 4087, 7045, 1900, 6692, 986, 5666, 4607] \n",
      "\n",
      "BoW vector size: 7851 \n",
      "\n",
      "TF-IDF vector size: 7851 \n",
      "\n",
      "Label:  tensor(1) \n",
      "\n",
      "Review text: CAT SOUP is a short anime based on the legendary manga Nekojiru. It won the award \\Best Short Film\\\" at The 6th Fantasia Film Festival and also won the \\\"Excellence Prize\\\" at Japan's Media Arts Festival.<br /><br />When little kitten Nyaako's soul is stolen by Death, she and her brother Nyatta embark on a bizarre journey to get it back. In the surreal dreamscape of the Other Side, they encounter many fantastic characters and remarkable, often disturbing adventures.<br /><br />CAT SOUP is an anime like nothing you've ever seen. It's Hello Kitty on acid! It is very original, stunningly beautiful and possess a great sense of strangeness and lyricism. CAT SOUP is very surrealistic (there are no dialogue) and sometimes cruel and gory. So it is more an anime for adults than children (they may not understand at all!). A great journey for those who get the chance to see this absolute masterpiece. An must-see!\" \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('number of training batches:', len(train_loader), '\\n')\n",
    "batch_idx = random.randint(0, len(train_loader)-1)\n",
    "example_idx = random.randint(0, BATCH_SIZE-1)\n",
    "\n",
    "for i, fields in enumerate(train_loader):\n",
    "    seq, bow, tfidf, target, text = fields\n",
    "    if i == batch_idx:\n",
    "        print('Training input sequence:', seq[example_idx], '\\n')\n",
    "        print('BoW vector size:', len(bow[example_idx]), '\\n')\n",
    "        print('TF-IDF vector size:', len(tfidf[example_idx]), '\\n')\n",
    "        print('Label: ', target[example_idx], '\\n')\n",
    "        print('Review text:', text[example_idx], '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build BoW Model\n",
    "\n",
    "- Input: BoW Vector\n",
    "- Model: \n",
    "    - feed-forward fully connected network\n",
    "    - 2 hidden layers\n",
    "- Output: \n",
    "    - vector size of 2 (2 possible outcome: positive v.s. negative)\n",
    "    - probability of input document classified as the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3., 4., 5.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BoWLogisticClassifier(nn.Module):\n",
    "    def __init__(self, device, vocab_size, output_size):\n",
    "        self.device = device\n",
    "        self.Linear = nn.Linear(vocab_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, device, vocab_size, hidden1, hidden2, num_labels, batch_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, num_labels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = len(x)\n",
    "        if batch_size != self.batch_size:\n",
    "            self.batch_size = batch_size\n",
    "        x = torch.FloatTensor(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.softmax(self.fc3(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoWClassifier(\n",
       "  (fc1): Linear(in_features=7851, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN1 = 100\n",
    "HIDDEN2 = 50\n",
    "\n",
    "bow_model = BoWClassifier(\n",
    "    vocab_size=len(dataset.token2idx),\n",
    "    hidden1=HIDDEN1,\n",
    "    hidden2=HIDDEN2,\n",
    "    num_labels=2,\n",
    "    device=device,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "bow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 7851])\n",
      "torch.Size([100])\n",
      "torch.Size([50, 100])\n",
      "torch.Size([50])\n",
      "torch.Size([2, 50])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for param in bow_model.parameters():\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Layer 1 affine: $$x_1 = W_1 X + b_1$$\n",
    "Layer 1 activation: $$h_1 = Relu(x_1)$$\n",
    "Layer 2 affine: $$x_2 = W_2 h_1 + b_2$$\n",
    "output: $$p = softmax(x_2)$$\n",
    "Loss: $$L = âˆ’(ylog(p)+(1âˆ’y)log(1âˆ’p))$$\n",
    "Gradient: \n",
    "$$\\frac{\\partial }{\\partial W_1}L(W_1, b_1, W_2, b_2) = \\frac{\\partial L}{\\partial p}\\frac{\\partial p}{\\partial x_2}\\frac{\\partial x_2}{\\partial h_1}\\frac{\\partial h_1}{\\partial x_1}\\frac{\\partial x_1}{\\partial W_1}$$\n",
    "\n",
    "Parameter update:\n",
    "$$W_1 = W_1 - \\alpha \\frac{\\partial L}{\\partial W_1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    ")\n",
    "scheduler = CosineAnnealingLR(optimizer, 1)\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer, train_loader, input_type='bow'):\n",
    "    model.train()\n",
    "    total_loss, total = 0, 0\n",
    "    for seq, bow, tfidf, target, text in train_loader:\n",
    "        inputs = bow\n",
    "        if input_type == 'tfidf':\n",
    "            inputs = tfidf\n",
    "        \n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Perform gradient descent, backwards pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Take a step in the right direction\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Record metrics\n",
    "        total_loss += loss.item()\n",
    "        total += len(target)\n",
    "\n",
    "    return total_loss / total\n",
    "\n",
    "\n",
    "def validate_epoch(model, valid_loader, input_type='bow'):\n",
    "    model.eval()\n",
    "    total_loss, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for seq, bow, tfidf, target, text in valid_loader:\n",
    "            inputs = bow\n",
    "            if input_type == 'tfidf':\n",
    "                inputs = tfidf\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(inputs)\n",
    "\n",
    "            # Calculate how wrong the model is\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Record metrics\n",
    "            total_loss += loss.item()\n",
    "            total += len(target)\n",
    "\n",
    "    return total_loss / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffrey/envs/pytorch-nlp-notebooks/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #  1\ttrain_loss: 1.31e-03\tvalid_loss: 1.34e-03\n",
      "\n",
      "epoch #  2\ttrain_loss: 1.31e-03\tvalid_loss: 1.34e-03\n",
      "\n",
      "epoch #  3\ttrain_loss: 1.31e-03\tvalid_loss: 1.34e-03\n",
      "\n",
      "epoch #  4\ttrain_loss: 1.31e-03\tvalid_loss: 1.34e-03\n",
      "\n",
      "Stopping early\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 0\n",
    "train_losses, valid_losses = [], []\n",
    "while True:\n",
    "    train_loss = train_epoch(bow_model, optimizer, train_loader, input_type='bow')\n",
    "    valid_loss = validate_epoch(bow_model, valid_loader, input_type='bow')\n",
    "    \n",
    "    tqdm.write(\n",
    "        f'epoch #{n_epochs + 1:3d}\\ttrain_loss: {train_loss:.2e}\\tvalid_loss: {valid_loss:.2e}\\n',\n",
    "    )\n",
    "    \n",
    "    # Early stopping if the current valid_loss is greater than the last three valid losses\n",
    "    if len(valid_losses) > 2 and all(valid_loss >= loss for loss in valid_losses[-3:]):\n",
    "        print('Stopping early')\n",
    "        break\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    n_epochs += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoWClassifier(\n",
       "  (fc1): Linear(in_features=7851, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN1 = 100\n",
    "HIDDEN2 = 50\n",
    "\n",
    "tfidf_model = BoWClassifier(\n",
    "    vocab_size=len(dataset.token2idx),\n",
    "    hidden1=HIDDEN1,\n",
    "    hidden2=HIDDEN2,\n",
    "    num_labels=2,\n",
    "    device=device,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "tfidf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffrey/envs/pytorch-nlp-notebooks/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #  1\ttrain_loss: 1.32e-03\tvalid_loss: 1.34e-03\n",
      "\n",
      "epoch #  2\ttrain_loss: 1.32e-03\tvalid_loss: 1.34e-03\n",
      "\n",
      "epoch #  3\ttrain_loss: 1.32e-03\tvalid_loss: 1.34e-03\n",
      "\n",
      "epoch #  4\ttrain_loss: 1.32e-03\tvalid_loss: 1.34e-03\n",
      "\n",
      "Stopping early\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 0\n",
    "train_losses, valid_losses = [], []\n",
    "while True:\n",
    "    train_loss = train_epoch(tfidf_model, optimizer, train_loader, input_type='tdidf')\n",
    "    valid_loss = validate_epoch(tfidf_model, valid_loader, input_type='tdidf')\n",
    "    \n",
    "    tqdm.write(\n",
    "        f'epoch #{n_epochs + 1:3d}\\ttrain_loss: {train_loss:.2e}\\tvalid_loss: {valid_loss:.2e}\\n',\n",
    "    )\n",
    "    \n",
    "    # Early stopping if the current valid_loss is greater than the last three valid losses\n",
    "    if len(valid_losses) > 2 and all(valid_loss >= loss for loss in valid_losses[-3:]):\n",
    "        print('Stopping early')\n",
    "        break\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    n_epochs += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffrey/envs/pytorch-nlp-notebooks/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.49      0.64      2893\n",
      "           1       0.07      0.49      0.12       214\n",
      "\n",
      "   micro avg       0.49      0.49      0.49      3107\n",
      "   macro avg       0.50      0.49      0.38      3107\n",
      "weighted avg       0.87      0.49      0.61      3107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bow_model.eval()\n",
    "test_accuracy, n_examples = 0, 0\n",
    "y_true, y_pred = [], []\n",
    "input_type = 'tfidf'\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq, bow, tfidf, target, text in test_loader:\n",
    "        inputs = bow\n",
    "        probs = bow_model(inputs)\n",
    "        if input_type == 'tdidf':\n",
    "            inputs = tfidf\n",
    "            probs = tfidf_model(inputs)\n",
    "        \n",
    "        probs = probs.detach().cpu().numpy()\n",
    "        predictions = np.argmax(probs, axis=1)\n",
    "        target = target.cpu().numpy()\n",
    "        \n",
    "        y_true.extend(predictions)\n",
    "        y_pred.extend(target)\n",
    "        \n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda x: [sublst for lst in x for sublst in lst]\n",
    "seq_lst, bow_lst, tfidf_lst, target_lst, text_lst = zip(*test_loader)\n",
    "seq_lst, bow_lst, tfidf_lst, target_lst, text_lst = map(flatten, [seq_lst, bow_lst, tfidf_lst, target_lst, text_lst])\n",
    "test_examples = list(zip(seq_lst, bow_lst, tfidf_lst, target_lst, text_lst))\n",
    "\n",
    "input_type = 'bow'\n",
    "\n",
    "def print_random_prediction(n=10):\n",
    "    to_emoji = lambda x: 'ðŸ˜„' if x else 'ðŸ˜¡'\n",
    "    model.eval()\n",
    "    rows = []\n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            seq, bow, tdidf, target, text = random.choice(test_examples)\n",
    "            target = target.item()\n",
    "            \n",
    "            inputs = bow\n",
    "            probs = bow_model([inputs])\n",
    "            if input_type == 'tdidf':\n",
    "                inputs = tfidf\n",
    "                probs = tfidf_model([inputs])\n",
    "            \n",
    "            probs = probs.detach().cpu().numpy()\n",
    "            prediction = np.argmax(probs, axis=1)[0]\n",
    "\n",
    "            predicted = to_emoji(prediction)\n",
    "            actual = to_emoji(target)\n",
    "            \n",
    "            row = f\"\"\"\n",
    "            <tr>\n",
    "            <td>{i+1}&nbsp;</td>\n",
    "            <td>{text}&nbsp;</td>\n",
    "            <td>{predicted}&nbsp;</td>\n",
    "            <td>{actual}&nbsp;</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "            rows.append(row)\n",
    "            \n",
    "    rows_joined = '\\n'.join(rows)\n",
    "    table = f\"\"\"\n",
    "<table>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td><b>Number</b>&nbsp;</td>\n",
    "<td><b>Review</b>&nbsp;</td>\n",
    "<td><b>Predicted</b>&nbsp;</td>\n",
    "<td><b>Actual</b>&nbsp;</td>\n",
    "</tr>\n",
    "{rows_joined}\n",
    "</tbody>\n",
    "</table>\n",
    "\"\"\"\n",
    "    display(HTML(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffrey/envs/pytorch-nlp-notebooks/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td><b>Number</b>&nbsp;</td>\n",
       "<td><b>Review</b>&nbsp;</td>\n",
       "<td><b>Predicted</b>&nbsp;</td>\n",
       "<td><b>Actual</b>&nbsp;</td>\n",
       "</tr>\n",
       "\n",
       "            <tr>\n",
       "            <td>1&nbsp;</td>\n",
       "            <td>great historical movie, will not allow a viewer to leave once you begin to watch. View is presented differently than displayed by most school books on this subject. My only fault for this movie is it was photographed in black and white; wished it had been in color ... wow !&nbsp;</td>\n",
       "            <td>ðŸ˜¡&nbsp;</td>\n",
       "            <td>ðŸ˜„&nbsp;</td>\n",
       "            </tr>\n",
       "            \n",
       "\n",
       "            <tr>\n",
       "            <td>2&nbsp;</td>\n",
       "            <td>Zeoy101?? Really, this has to be one of the most stupidest attempts to get people in my age group's attention. It's about some preppy girl named Zeoy and her friends that attends boarding school. BORING!!! All she ever does is whine and complain and acts like a spoiled idiot. I remember this show came out in 2005, I was 13 going on 14, and even then I thought it was pointless. The only episode I EVER liked was when the boys hid a camera in the girls dorm. THAT'S IT. Anyway, I just don't understand why Nickel-Oh my bad-Nick feels the need to syndicate this sorry poor excuse for \"entertainment\". serious this decade is becoming a joke every year and it gets worst and worst. What's with this generation??<br /><br />Anyway, R.I.P. Nickelodeon 1979-1998?/2005?&nbsp;</td>\n",
       "            <td>ðŸ˜¡&nbsp;</td>\n",
       "            <td>ðŸ˜¡&nbsp;</td>\n",
       "            </tr>\n",
       "            \n",
       "\n",
       "            <tr>\n",
       "            <td>3&nbsp;</td>\n",
       "            <td>As a cinema fan White Noise was an utter disappointment, as a filmmaker the cinematography was pretty good, nicely lit, good camera work, reasonable direction. But as a film it just seamed as predictable as all the other 'so called' horror movies that the market has recently been flooded with. Although it did have a little bit of the 'chill factor' the whole concept of the E.V.O (Electronic Voice Phenomena) did'not seem believable. This movie did not explain the reasonings for certain occurrences but went ahead with them. The acting was far from mind blowing the main character portrayed no emotion, like many recent thriller/horror movies.<br /><br />Definitely not a movie I will be buying on DVD and would not recommend anyone rushes out to see it.&nbsp;</td>\n",
       "            <td>ðŸ˜¡&nbsp;</td>\n",
       "            <td>ðŸ˜¡&nbsp;</td>\n",
       "            </tr>\n",
       "            \n",
       "\n",
       "            <tr>\n",
       "            <td>4&nbsp;</td>\n",
       "            <td>This film is about a woman falling in love with a friend of her boyfriend. From then on, she has to divide her time for the two boyfriends: Jack during the day and Joseph during the night.<br /><br />This film feels like as if it was made with minimum budget. The majority of the film is set in a flat with minimal furniture. There are only three main actors, all the other actors listed in the credits make only momentary appearances. The wardrobe designer doesn't seem to have much to do, as the actors wear very down to earth clothes, and actually most of the time they are naked anyway.<br /><br />The film is very dialog heavy, which should have made up for the shortcomings described above. However, the dialogs sound too composed and awkward. In the beginning of the film, most of the dialog is a person saying a very long sentence, and then the person says 'Me too'. After the frenzy of agreement, the dialog descends into a mess of disjointed and confused word salad.<br /><br />The only merit of this film I can think of is that it serves as a feminist outlet which conveys that it is not just men who can be unfaithful.<br /><br />This film is a great disappointment.&nbsp;</td>\n",
       "            <td>ðŸ˜¡&nbsp;</td>\n",
       "            <td>ðŸ˜¡&nbsp;</td>\n",
       "            </tr>\n",
       "            \n",
       "\n",
       "            <tr>\n",
       "            <td>5&nbsp;</td>\n",
       "            <td>The film starts with a voice over telling the audience where they are, and who the characters are. And that is the moment i started to dislike the movie. With all the endless possibilities any film director have in hand, i really find it a very easy and cheap solution to express the situation with a voice over telling everything. I actually believe voice overs are betrayals to the film making concept.<br /><br />I hate to hear from a voice over saying where we are, which date we are at, and especially what the characters feel and think. I believe that a director has to find a visual way to transmit the feelings and the thoughts of the characters to the audience. <br /><br />But after the bad influencing intro, a very striking movie begins and keeps going for a fairly long enough time. The lives of a middle class family and all the members individually are depicted in a perfect realistic way. I think the director has a talent for capturing real life situations. For example, a father who has to make his private calls from the bathroom might seem abnormal at first, but life itself leads us some situations which might seem abnormal but also very normal as well. I think the director is a very good observer about real life.<br /><br />But that is it. After a while the realism in the movie begins to sacrifice the story-telling. I really felt like I'm having a big headache because of the non-stop talking characters. It was as if the actors and actresses were given the subject and were allowed to improvise the dialogs. It is realistic really, but characters always asking \"really, is that so\" etc. to each other, or characters saying \"no\" or \"are you listening to me,\" ten times when saying it only once is just enough causes me to have a headache.<br /><br />I also think the play practicing and book reading scenes are more then they should be. I understand that the play and the book in the movie are very much related to the plot, but i think the director has missed the point where he should stop showing these scenes.&nbsp;</td>\n",
       "            <td>ðŸ˜¡&nbsp;</td>\n",
       "            <td>ðŸ˜¡&nbsp;</td>\n",
       "            </tr>\n",
       "            \n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_random_prediction(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-nlp-notebooks",
   "language": "python",
   "name": "pytorch-nlp-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
