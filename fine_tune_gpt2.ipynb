{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from boltons.iterutils import windowed\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import torch\n",
    "from pytorch_pretrained_bert import GPT2Tokenizer, GPT2LMHeadModel, OpenAIAdam\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune the GPT-2 model with weight loss articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_text(model, seed='Weight loss can be achieved by ', n_words=500, temperature=1.0):\n",
    "    model.eval()\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    text = tokenizer.encode(seed)\n",
    "    inputs, past = torch.tensor([text]), None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm_notebook(range(n_words), leave=False):\n",
    "            logits, past = model(inputs.to(device), past=past)\n",
    "\n",
    "            # TODO: Add temperature for better results\n",
    "            log_probs = F.softmax(logits[:, -1], dim=-1)\n",
    "            inputs = torch.multinomial(log_probs, 1)\n",
    "            \n",
    "            text.append(inputs.item())\n",
    "\n",
    "    print(tokenizer.decode(text), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad605b79cdf40778942e473af61c12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weight loss can be achieved by  offering the shares a fair value . Investment associations introduced this scheme in the 1990s to simplify Securities Licensing very quickly, and I recall them reviewing their version in 2002.\n",
      "\n",
      "Look who's giving up their comfortable legal cushion to golfer vs 00 Dive tech, try playing in 20's physics mass ellipses , 20's curves etc. realize commanding position in certain golf courses is ridiculous , don't give up the ability to hit your target. Shut four years idea and not do them any favors. they take over only Wednesday running business to get to what you have to do and businesses they're doing what?? Free your energy down the pipeline when you want maximize profits and best off call infact. it all boils down to your knee Agricultural beef is environmentally sensitive because, toxic weed killers and nutrients are also a big risk, livelihood risk and heavy metals material don't go into the lungs or shipfrom containers then the failure risk that banks would turn off or a government spokesman would pull deep due to poor or wilful citizenry. excerpt from what just happens at most OTM:REGION TO PROCEED THOUGHTS which runs like this on 06/14, 2011 at 4:09 pm: direct comment Average Seed Prices 49 States $56,221,080 Actual Profit 1 Billion 3 Billion $3 Billion $1000,000,000,000 Country $77,255,756 Countries $151,452,100 IndexOfLife Schedule 1880-19% 2001-10% 'mean basket link Percentage of Individuals doing what 270,388 years from now? less 1% 8% 20% 50% 75% 100% 50%\n",
      "\n",
      "14. Authorized Contact 39,627 http://www.gdc.org/\n",
      "\n",
      "15. Backstops Fire Each Main Building & 150 Main Street CC New York Customers Can Work For 28 Months on Buy-Up Backup Rooms Recycling & Voting What has the most pressing issue you're concerned about right now? Equality of opportunity and financial security. First Aid always seems to be more important than standing up for your rights and a time to lose weight.\n",
      "\n",
      "\n",
      "Life Worthy Of Vital Issues Vouchers by Email and Company View StartUrows with My Profile! http://bit.ly/0Xmt4Y4 (Based Features for Everyone Add 16 Invalidated Medium Email Requirements to\n",
      "\n",
      "Got Slow A Body So Diet and Fit The Metabolism Can Boost Your Levels of Seduction 5. Top of"
     ]
    }
   ],
   "source": [
    "sample_text(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what the fine-tuning data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jesse L Moore</td>\n",
       "      <td>It is almost not possible to watch any TV, rea...</td>\n",
       "      <td>How Obesity is Determined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Erica Logan</td>\n",
       "      <td>If you are reading this article, then I know y...</td>\n",
       "      <td>I Cheated My Way Thin - I Can Now Look At Myse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acharya Hargreaves</td>\n",
       "      <td>Self hypnosis for weight loss is a very easy p...</td>\n",
       "      <td>Self Hypnosis For Weight Loss is Easy and Rela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avy Barnes</td>\n",
       "      <td>Are you looking all over for the fastest way t...</td>\n",
       "      <td>Fastest Way to Lose Weight - Melt Away Lbs Of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carolyn Anderson</td>\n",
       "      <td>Losing weight is one of the many concerns of m...</td>\n",
       "      <td>How to Lose Weight the Healthy Way - Simple an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                               text  \\\n",
       "0       Jesse L Moore  It is almost not possible to watch any TV, rea...   \n",
       "1         Erica Logan  If you are reading this article, then I know y...   \n",
       "2  Acharya Hargreaves  Self hypnosis for weight loss is a very easy p...   \n",
       "3          Avy Barnes  Are you looking all over for the fastest way t...   \n",
       "4    Carolyn Anderson  Losing weight is one of the many concerns of m...   \n",
       "\n",
       "                                               title  \n",
       "0                          How Obesity is Determined  \n",
       "1  I Cheated My Way Thin - I Can Now Look At Myse...  \n",
       "2  Self Hypnosis For Weight Loss is Easy and Rela...  \n",
       "3  Fastest Way to Lose Weight - Melt Away Lbs Of ...  \n",
       "4  How to Lose Weight the Healthy Way - Simple an...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTICLES_PATH = 'ezine/health_and_fitness/weight_loss/weight_loss-articles-all.json'\n",
    "df = pd.read_json(ARTICLES_PATH)[['author', 'text', 'title']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess fine-tuning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda x: [sublst for lst in x for sublst in lst]\n",
    "to_sentences = lambda text, nlp: [sentence.text for sentence in nlp(text).sents]\n",
    "\n",
    "class EzineWeightLossDataset(Dataset):\n",
    "    \"\"\"Weight loss articles from ezinearticles.com.\"\"\"\n",
    "    def __init__(self, data_filename, sequence_length=128):\n",
    "        nlp = spacy.load('en_core_web_md')\n",
    "        \n",
    "        df = pd.read_json(data_filename)[['author', 'text', 'title']]\n",
    "        \n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        \n",
    "        # TODO: preserve newlines after each paragraph\n",
    "        # (last sentence needs a newline character at the end)\n",
    "\n",
    "        # Convert each article into a list of sentences\n",
    "        # using SpaCy to find sentence boundaries\n",
    "        articles_sentences = df.iloc[:100].text.progress_apply(  # TODO: remove limit\n",
    "            partial(to_sentences, nlp=nlp),\n",
    "        )\n",
    "        \n",
    "        # Apply GPT-2 encoding to each sentence.\n",
    "        # Then, flatten sentences per article so that each article\n",
    "        # is just a list of token indices\n",
    "        encoded_articles = [\n",
    "            flatten([self.tokenizer.encode(sentence) for sentence in article_sentences])\n",
    "            for article_sentences in tqdm(articles_sentences)\n",
    "        ]\n",
    "        \n",
    "        # Apply a sliding window per article that will be the sequence\n",
    "        # length fed into the model\n",
    "        sequences = flatten([\n",
    "            windowed(encoded_article, sequence_length)\n",
    "            for encoded_article in encoded_articles\n",
    "        ])\n",
    "        \n",
    "        # Combine all of the sequences into one 2-D matrix.\n",
    "        # Then, split like [A, B, C, D, E] --> ([A, B, C, D], [B, C, D, E])\n",
    "        data = torch.tensor(sequences)\n",
    "        self.inputs_lst, self.targets = data[:-1], data[1:]\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.inputs_lst[i], self.targets[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.32it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 208.13it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = EzineWeightLossDataset(ARTICLES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "loader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the GPT-2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from: https://github.com/CQCumbers/grifbot/blob/83e3f9e434f4e2acbb527cb322d621be0aeb646d/scripts/gpt2.py#L61\n",
    "# TODO: cleanup\n",
    "n_epochs = 1\n",
    "learning_rate = 6.25e-10\n",
    "warmup_proportion = 0.002\n",
    "max_grad_norm = 0.05\n",
    "weight_decay = 0.01\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if\n",
    "        not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if\n",
    "        any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "n_train_optimization_steps = len(dataset) * n_epochs // batch_size\n",
    "optimizer = OpenAIAdam(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=learning_rate,\n",
    "    warmup=warmup_proportion,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    weight_decay=weight_decay,\n",
    "    t_total=n_train_optimization_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979cedd5ddee4d5bb8b1ec099d707b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e17d9d3c614e5abc8ddb0d562bc37b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=5552, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Code taken from: https://github.com/CQCumbers/grifbot/blob/83e3f9e434f4e2acbb527cb322d621be0aeb646d/scripts/gpt2.py#L77\n",
    "# TODO: cleanup\n",
    "nb_tr_steps, tr_loss, exp_average_loss = 0, 0, None\n",
    "model.train()\n",
    "for _ in tqdm_notebook(range(n_epochs)):\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    tqdm_bar = tqdm_notebook(loader, desc='Training')\n",
    "    for step, batch in enumerate(tqdm_bar):\n",
    "        input_ids, lm_labels = tuple(t.to(device) for t in batch)\n",
    "        loss = model(input_ids, lm_labels=lm_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "        exp_average_loss = (\n",
    "            loss.item() if exp_average_loss is None\n",
    "            else 0.7 * exp_average_loss + 0.3 * loss.item()\n",
    "        )\n",
    "        nb_tr_steps += 1\n",
    "        tqdm_bar.desc = f'Training loss: {exp_average_loss:.2e} lr: {optimizer.get_lr()[0]:.2e}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "original_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Weight loss can be achieved by eringing diets and changes in caloric intake, although approximately one-third of the diet-moderator carbohydrate-supplemented humans who are subjects of frequent sudden cardiac arrest do not achieve the full 16% to 17% of sustained ketide-induced insulin resistance.22 , 23 , 24 Consumption of fatty acids in decreasing percentages of weight and body strength has been associated with decreased protein and fat composition during recovery, but no a priori evidence of detrimental effects on muscle metabolism and dysfunction.25 The author noted that it may be therefore necessary to program for preterm that when intensity is within a normal range, lean body mass will degrade. In this theory, bold predictive nutrition bias should be introduced to adjust protein-energy composition above the lower bound of 1.8 grams of protein and lower or not at all to calories available at low glycogen concentrations; using additions merely to compensate for weight loss (and other factors) might benefit leaners and willing participants.\n",
      "\n",
      "As such, all food-borne inflammation and cardio-protective properties can be acknowledged, such as reduced heart rate of Post-exercise Premium Nouni-Do-Your-Part part 1max submission blood glucose levels, and imparasitic arterial inflammation and pulmonary arterial hypertension, indicating that dietary disruption of lipid Molten-True collagen synthesis is important.26\n",
      "\n",
      "Differences between the protein synthesis and function of the liver's‐cardiovascular system are apparent even when changes in dietary protein intake are maintained during a challenge process. In bouts of muscle self‐renewal occurring during the protein synthesis, patients on insulin-resistant diets gain relative weight (0.05 compared to a control group ± 0.05 kg) with histologically similar body mass indexes and mean lifespan without surgery.27 Serum monocyte‐stimulating hormone concentrations in humans bioequivalently decreased by 35-45% after dietary administration of 3 h of high‐fructose corn syrup (HFCS), 33 that are important for controlling fatigue‐induced insulin resistance in lean young humans.28, 29 The authors also indicated that the transient conductance (CT) elevation in protein as well as in changes in production and production independent of weight by exercise was below the forty‐minute‐to‐3 hour sampling tolerability advertised in rodents and circadian rhythms (Figure 2).30\n",
      "\n",
      "Also, a commonly‐supported assumption in adaptive literature studies is that physiologically new body mass determines adorables less than equivalent nicotinic‐"
     ]
    }
   ],
   "source": [
    "# Sample text from the original model\n",
    "sample_text(original_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Weight loss can be achieved by  homebrewing and homebrewing preserve many homebrew recipes into small biological products during long-term brewing process. When a multidimensional using kitchen has used this method for esamazing drum rocks and better mixing patterns, it depends on internally and externally derived laws. The resulting aromas from the malt extract and reducing potency of maltstouts varies depending around the humidity limit or play current time, best physical interpretation, mimicking the olfactory profile of nitroglycerin and lactose developed with reference to the state & fishery population in bulk. Analysis of proteins containing this compound on array can explain results, indicating energy property of grains in effect with 3, 5 & 10% slurry off balance. The experts say folic acid and baking soda. Certain sugar extractors can cut down on salinity due to maturation time coefficients (instead of forming natural pH in natural intermediate water). Lactose distillation beleives doped with vinylmolymer in state & fishery water often work better. The Negotiable Bitter flavor Flavor of Nautic Dry hard flavors newer than Neumac 6.115, 8-ick humidorous to iso-tap, electrolytes be present , and low with temperature (lower by holding recipe temperature higher than what one should ferment due to reagent and fermentation temperature. Reduce pH temp with yeast. ) Mixing with water at the same time alters final temp, a story often by BI raises the fermentation temperature. Bread peel flavor of Beta-lactol, Weitild\n",
      "Aucimbic Node Ingredients: (1) Hasta-Natural guar gum 1.5 lbs Glove fir of GPSL Salt dissolve sense 13; LH Brassica & resin , 7.64 Ultimatum Havana Extract. 1 1/2 lbs Estad C'mondol, Daniels & Petty . 5 Li Constitol, Dow-TM Soya. 1.5 tsp. Gouda Benadryl 100%, 2 Cdobenas Liquid Dimethyluble Dicnl Color typing guide clearer Rt Morary\n",
      "Advice available in 11 (Lk := 0/0.166 ac).[ 1. Location of spoilage source unknowns, eg. the USDA but this decide that Lg8_request 0.25% amines pay . Local ales seem to come in (Lknowize Currently). [ 1. Collaborate.) Reagent, carbohydrate 3yol, ammonidino acid"
     ]
    }
   ],
   "source": [
    "# Sample text from the fine-tuned model\n",
    "sample_text(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'finetuned_gpt2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
